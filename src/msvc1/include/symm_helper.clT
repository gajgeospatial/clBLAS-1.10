/* ************************************************************************
 * Copyright 2013 Advanced Micro Devices, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * ************************************************************************/

static const char *SYMM_HEMM_HELPER = "\
#ifdef DOUBLE_PRECISION\n\
    #ifdef cl_khr_fp64\n\
    #pragma OPENCL EXTENSION cl_khr_fp64 : enable\n\
    #else\n\
    #pragma OPENCL EXTENSION cl_amd_fp64 : enable\n\
    #endif\n\
#endif\n\
\n\
%TYPE SYMM_SCALAR_LOAD(__global %TYPE const * restrict A, uint M, uint lda, uint row, uint col)\n\
{\n\
    %TYPE retval;\n\
\n\
    //PENDING: Remove this Check for M. This will never happen\n\
    if (((row) < M) && (col < M))\n\
    {\n\
        #ifdef __SYMM_UPPER__\n\
        if ((row) <= col)\n\
        #else\n\
        if ((row) >= col)\n\
        #endif\n\
        {\n\
            retval = A[(col)*lda + row];\n\
            #ifdef __HEMM__\n\
            if (row == col) { retval.odd = 0; }\n\
            #endif\n\
        } else {\n\
            retval = A[(row)*lda + col];\n\
            #ifdef __HEMM__\n\
            %CONJUGATE(1, retval);\n\
            #endif\n\
        }\n\
    } else {\n\
        retval = (%TYPE) 0;\n\
    }\n\
    return retval;\n\
}\n\
\n\
%TYPE%V SYMM_VECTOR_LOAD_USING_SCALAR(__global %TYPE const * restrict A, uint M, uint lda, uint row, uint col)\n\
{\n\
    //%TYPE symm_vec_load_temp[%V];\n\
    %TYPE%V symm_vec_retval;\n\
\n\
    //#pragma unroll %V\n\
    //for(uint index_i=0; index_i< (%V); index_i++)\n\
    %VFOR\n\
    {\n\
        //PENDING: Remove this Check for M. This will never happen\n\
        if (((row + %VFORINDEX) < M) && (col < M))\n\
        {\n\
"
"\
            #ifdef __SYMM_UPPER__\n\
            if ((row + %VFORINDEX) <= col)\n\
            #else\n\
            if ((row + %VFORINDEX) >= col)\n\
            #endif\n\
            {\n\
                //symm_vec_load_temp[index_i] = A[(col)*(lda) + ((row) + index_i)];\n\
                symm_vec_retval%VFORSUFFIX = A[(col)*(lda) + ((row) + %VFORINDEX)];\n\
                #ifdef __HEMM__\n\
                //if ((row + index_i) == col) { symm_vec_load_temp[index_i].odd = 0; }\n\
                if ((row + %VFORINDEX) == col) { (symm_vec_retval%VFORSUFFIX).odd = 0; }\n\
                #endif\n\
            } else {\n\
                //symm_vec_load_temp[index_i] = A[((row)+index_i)*(lda) + (col)];\n\
                symm_vec_retval%VFORSUFFIX = A[((row)+ %VFORINDEX )*(lda) + (col)];\n\
                #ifdef __HEMM__\n\
                //CONJUGATE(1, (symm_vec_load_temp[index_i]));\n\
                {\n\
                    %TYPE SCALAR;\n\
\n\
                    SCALAR = symm_vec_retval%VFORSUFFIX;\n\
                    %CONJUGATE(1, SCALAR);\n\
                    symm_vec_retval%VFORSUFFIX = SCALAR;\n\
                }\n\
                #endif\n\
            }\n\
        } else {\n\
            //symm_vec_load_temp[index_i] = (%TYPE) 0;\n\
            symm_vec_retval%VFORSUFFIX = (%TYPE) 0;\n\
        }\n\
    }\n\
    //%VLOADWITHINCX(symm_vec_retval, symm_vec_load_temp, 1 );\n\
    //symm_vec_retval = *(__private %TYPE%V *)symm_vec_load_temp;\n\
    return symm_vec_retval;\n\
}\n\
\n";



